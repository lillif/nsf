defaults:
   - datamodule
   - model
   - wandb

# -------- Wandb logging --------
experiment_name: nsp-tropics_small-goes # Define experiment name

tags: neural_spline_flows goes-16 tropics_small # Define tags for wandb logging

log_cmd_wandb: True # Log command to wand
log_config_as: yaml # Log yaml config to wandb
log_metrics: True # to log acc, loss, f1, conf mat to wandb and save corresponding checkpoints

log_every_n_steps: 50
monitor_metric: val/loss  # Decide on metric to log

# -------- Hydra Output Directory --------
now_dir: ${now:%Y-%m-%d}/${now:%H-%M-%S}

hydra:
  run:
    dir: /work/bb1153/b382145/computer_vision/hydra/outputs/${experiment_name}/${now_dir}

# -------- Training details  --------
gpus: 1
max_epochs: 75 # Define number of training epochs

seed: 42 # Define experiment seed - only use torch random operations (which will be seeded) for reproducibility

precision: 32-true # Define float precision
accumulate_grad_batches: 1 # Define whether to accumulate gradients before running optimizer
use_deterministic_algorithms: False # Define whether to use deterministic algorithms
strategy: auto # ddp for multi-gpu, auto or single_device for single-gpu, 
num_nodes: 1 # Define number of GPU nodes for distributed training

# -------- Data details  --------
limit_train_batches: 1.0 # Limits the amount of training data. Selection random and changing across epochs.
limit_val_batches: 1.0 # needs to be one, otherwise only ifs data is removed, then icon, then goes!
limit_test_batches: 1.0 # needs to be one, same reason as above