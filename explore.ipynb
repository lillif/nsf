{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e94d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1819255/2807664168.py:5: DeprecationWarning: scipy.misc is deprecated and will be removed in 2.0.0\n",
      "  import scipy.misc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import sacred\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from sacred import Experiment, observers\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from experiments import autils\n",
    "# from experiments.autils import Conv2dSameSize, LogProbWrapper\n",
    "from experiments.images_data import get_data, Preprocess\n",
    "\n",
    "from data import load_num_batches\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from nde import distributions, transforms, flows\n",
    "import utils\n",
    "import optim\n",
    "import nn as nn_\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b5b7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvNet(nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "#         super().__init__()\n",
    "#         self.hidden_channels = hidden_channels\n",
    "#         self.net = nn.Sequential(\n",
    "#             Conv2dSameSize(in_channels, hidden_channels, kernel_size=3),\n",
    "#             nn.ReLU(),\n",
    "#             Conv2dSameSize(hidden_channels, hidden_channels, kernel_size=1),\n",
    "#             nn.ReLU(),\n",
    "#             Conv2dSameSize(hidden_channels, out_channels, kernel_size=3),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, inputs, context=None):\n",
    "#         return self.net.forward(inputs)\n",
    "\n",
    "# # @ex.capture\n",
    "# def create_transform_step(num_channels,\n",
    "#                           hidden_channels, actnorm, coupling_layer_type, spline_params,\n",
    "#                           use_resnet, num_res_blocks, resnet_batchnorm, dropout_prob):\n",
    "#     if use_resnet:\n",
    "#         def create_convnet(in_channels, out_channels):\n",
    "#             net = nn_.ConvResidualNet(in_channels=in_channels,\n",
    "#                                       out_channels=out_channels,\n",
    "#                                       hidden_channels=hidden_channels,\n",
    "#                                       num_blocks=num_res_blocks,\n",
    "#                                       use_batch_norm=resnet_batchnorm,\n",
    "#                                       dropout_probability=dropout_prob)\n",
    "#             return net\n",
    "#     else:\n",
    "#         if dropout_prob != 0.:\n",
    "#             raise ValueError()\n",
    "#         def create_convnet(in_channels, out_channels):\n",
    "#             return ConvNet(in_channels, hidden_channels, out_channels)\n",
    "\n",
    "#     mask = utils.create_mid_split_binary_mask(num_channels)\n",
    "\n",
    "#     if coupling_layer_type == 'cubic_spline':\n",
    "#         coupling_layer = transforms.PiecewiseCubicCouplingTransform(\n",
    "#             mask=mask,\n",
    "#             transform_net_create_fn=create_convnet,\n",
    "#             tails='linear',\n",
    "#             tail_bound=spline_params['tail_bound'],\n",
    "#             num_bins=spline_params['num_bins'],\n",
    "#             apply_unconditional_transform=spline_params['apply_unconditional_transform'],\n",
    "#             min_bin_width=spline_params['min_bin_width'],\n",
    "#             min_bin_height=spline_params['min_bin_height']\n",
    "#         )\n",
    "#     elif coupling_layer_type == 'quadratic_spline':\n",
    "#         coupling_layer = transforms.PiecewiseQuadraticCouplingTransform(\n",
    "#             mask=mask,\n",
    "#             transform_net_create_fn=create_convnet,\n",
    "#             tails='linear',\n",
    "#             tail_bound=spline_params['tail_bound'],\n",
    "#             num_bins=spline_params['num_bins'],\n",
    "#             apply_unconditional_transform=spline_params['apply_unconditional_transform'],\n",
    "#             min_bin_width=spline_params['min_bin_width'],\n",
    "#             min_bin_height=spline_params['min_bin_height']\n",
    "#         )\n",
    "#     elif coupling_layer_type == 'rational_quadratic_spline':\n",
    "#         coupling_layer = transforms.PiecewiseRationalQuadraticCouplingTransform(\n",
    "#             mask=mask,\n",
    "#             transform_net_create_fn=create_convnet,\n",
    "#             tails='linear',\n",
    "#             tail_bound=spline_params['tail_bound'],\n",
    "#             num_bins=spline_params['num_bins'],\n",
    "#             apply_unconditional_transform=spline_params['apply_unconditional_transform'],\n",
    "#             min_bin_width=spline_params['min_bin_width'],\n",
    "#             min_bin_height=spline_params['min_bin_height'],\n",
    "#             min_derivative=spline_params['min_derivative']\n",
    "#         )\n",
    "#     elif coupling_layer_type == 'affine':\n",
    "#         coupling_layer = transforms.AffineCouplingTransform(\n",
    "#             mask=mask,\n",
    "#             transform_net_create_fn=create_convnet\n",
    "#         )\n",
    "#     elif coupling_layer_type == 'additive':\n",
    "#         coupling_layer = transforms.AdditiveCouplingTransform(\n",
    "#             mask=mask,\n",
    "#             transform_net_create_fn=create_convnet\n",
    "#         )\n",
    "#     else:\n",
    "#         raise RuntimeError('Unknown coupling_layer_type')\n",
    "\n",
    "#     step_transforms = []\n",
    "\n",
    "#     if actnorm:\n",
    "#         step_transforms.append(transforms.ActNorm(num_channels))\n",
    "\n",
    "#     step_transforms.extend([\n",
    "#         transforms.OneByOneConvolution(num_channels),\n",
    "#         coupling_layer\n",
    "#     ])\n",
    "\n",
    "#     return transforms.CompositeTransform(step_transforms)\n",
    "\n",
    "\n",
    "# # @ex.capture\n",
    "# def create_transform(c, h, w,\n",
    "#                      levels, hidden_channels, steps_per_level, alpha, num_bits, preprocessing,\n",
    "#                      multi_scale):\n",
    "#     if not isinstance(hidden_channels, list):\n",
    "#         hidden_channels = [hidden_channels] * levels\n",
    "\n",
    "#     if multi_scale:\n",
    "#         mct = transforms.MultiscaleCompositeTransform(num_transforms=levels)\n",
    "#         for level, level_hidden_channels in zip(range(levels), hidden_channels):\n",
    "#             squeeze_transform = transforms.SqueezeTransform()\n",
    "#             c, h, w = squeeze_transform.get_output_shape(c, h, w)\n",
    "\n",
    "#             transform_level = transforms.CompositeTransform(\n",
    "#                 [squeeze_transform]\n",
    "#                 + [create_transform_step(c, level_hidden_channels) for _ in range(steps_per_level)]\n",
    "#                 + [transforms.OneByOneConvolution(c)] # End each level with a linear transformation.\n",
    "#             )\n",
    "\n",
    "#             new_shape = mct.add_transform(transform_level, (c, h, w))\n",
    "#             if new_shape:  # If not last layer\n",
    "#                 c, h, w = new_shape\n",
    "#     else:\n",
    "#         all_transforms = []\n",
    "\n",
    "#         for level, level_hidden_channels in zip(range(levels), hidden_channels):\n",
    "#             squeeze_transform = transforms.SqueezeTransform()\n",
    "#             c, h, w = squeeze_transform.get_output_shape(c, h, w)\n",
    "\n",
    "#             transform_level = transforms.CompositeTransform(\n",
    "#                 [squeeze_transform]\n",
    "#                 + [create_transform_step(c, level_hidden_channels) for _ in range(steps_per_level)]\n",
    "#                 + [transforms.OneByOneConvolution(c)] # End each level with a linear transformation.\n",
    "#             )\n",
    "#             all_transforms.append(transform_level)\n",
    "\n",
    "#         all_transforms.append(transforms.ReshapeTransform(\n",
    "#             input_shape=(c,h,w),\n",
    "#             output_shape=(c*h*w,)\n",
    "#         ))\n",
    "#         mct = transforms.CompositeTransform(all_transforms)\n",
    "\n",
    "#     # Inputs to the model in [0, 2 ** num_bits]\n",
    "\n",
    "#     if preprocessing == 'glow':\n",
    "#         # Map to [-0.5,0.5]\n",
    "#         preprocess_transform = transforms.AffineScalarTransform(scale=(1. / 2 ** num_bits),\n",
    "#                                                                 shift=-0.5)\n",
    "#     elif preprocessing == 'realnvp':\n",
    "#         preprocess_transform = transforms.CompositeTransform([\n",
    "#             # Map to [0,1]\n",
    "#             transforms.AffineScalarTransform(scale=(1. / 2 ** num_bits)),\n",
    "#             # Map into unconstrained space as done in RealNVP\n",
    "#             transforms.AffineScalarTransform(shift=alpha,\n",
    "#                                              scale=(1 - alpha)),\n",
    "#             transforms.Logit()\n",
    "#         ])\n",
    "\n",
    "#     elif preprocessing == 'realnvp_2alpha':\n",
    "#         preprocess_transform = transforms.CompositeTransform([\n",
    "#             transforms.AffineScalarTransform(scale=(1. / 2 ** num_bits)),\n",
    "#             transforms.AffineScalarTransform(shift=alpha,\n",
    "#                                              scale=(1 - 2. * alpha)),\n",
    "#             transforms.Logit()\n",
    "#         ])\n",
    "#     else:\n",
    "#         raise RuntimeError('Unknown preprocessing type: {}'.format(preprocessing))\n",
    "\n",
    "#     return transforms.CompositeTransform([preprocess_transform, mct])\n",
    "\n",
    "# # @ex.capture\n",
    "# def create_flow(c, h, w,\n",
    "#                 flow_checkpoint, _log):\n",
    "#     distribution = distributions.StandardNormal((c * h * w,))\n",
    "#     transform = create_transform(c, h, w)\n",
    "\n",
    "#     flow = flows.Flow(transform, distribution)\n",
    "\n",
    "#     _log.info('There are {} trainable parameters in this model.'.format(\n",
    "#         utils.get_num_parameters(flow)))\n",
    "\n",
    "#     if flow_checkpoint is not None:\n",
    "#         flow.load_state_dict(torch.load(flow_checkpoint))\n",
    "#         _log.info('Flow state loaded from {}'.format(flow_checkpoint))\n",
    "\n",
    "#     return flow\n",
    "\n",
    "# # @ex.capture\n",
    "# def train_flow(flow, train_dataset, val_dataset, dataset_dims, device,\n",
    "#                batch_size, num_steps, learning_rate, cosine_annealing, warmup_fraction,\n",
    "#                temperatures, num_bits, num_workers, intervals, multi_gpu, actnorm,\n",
    "#                optimizer_checkpoint, start_step, eta_min, _log):\n",
    "#     run_dir = fso.dir\n",
    "\n",
    "#     flow = flow.to(device)\n",
    "\n",
    "#     summary_writer = SummaryWriter(run_dir, max_queue=100)\n",
    "\n",
    "#     train_loader = DataLoader(dataset=train_dataset,\n",
    "#                               batch_size=batch_size,\n",
    "#                               num_workers=num_workers)\n",
    "\n",
    "#     if val_dataset:\n",
    "#         val_loader = DataLoader(dataset=val_dataset,\n",
    "#                                 batch_size=batch_size,\n",
    "#                                 num_workers=num_workers)\n",
    "#     else:\n",
    "#         val_loader = None\n",
    "\n",
    "#     # Random batch and identity transform for reconstruction evaluation.\n",
    "#     random_batch, _ = next(iter(DataLoader(\n",
    "#         dataset=train_dataset,\n",
    "#         batch_size=batch_size,\n",
    "#         num_workers=0 # Faster than starting all workers just to get a single batch.\n",
    "#     )))\n",
    "#     identity_transform = transforms.CompositeTransform([\n",
    "#         flow._transform,\n",
    "#         transforms.InverseTransform(flow._transform)\n",
    "#     ])\n",
    "\n",
    "#     optimizer = torch.optim.Adam(flow.parameters(), lr=learning_rate)\n",
    "\n",
    "#     if optimizer_checkpoint is not None:\n",
    "#         optimizer.load_state_dict(torch.load(optimizer_checkpoint))\n",
    "#         _log.info('Optimizer state loaded from {}'.format(optimizer_checkpoint))\n",
    "\n",
    "#     if cosine_annealing:\n",
    "#         if warmup_fraction == 0.:\n",
    "#             scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#                 optimizer=optimizer,\n",
    "#                 T_max=num_steps,\n",
    "#                 last_epoch=-1 if start_step == 0 else start_step,\n",
    "#                 eta_min=eta_min\n",
    "#             )\n",
    "#         else:\n",
    "#             scheduler = optim.CosineAnnealingWarmUpLR(\n",
    "#                 optimizer=optimizer,\n",
    "#                 warm_up_epochs=int(warmup_fraction * num_steps),\n",
    "#                 total_epochs=num_steps,\n",
    "#                 last_epoch=-1 if start_step == 0 else start_step,\n",
    "#                 eta_min=eta_min\n",
    "#             )\n",
    "#     else:\n",
    "#         scheduler = None\n",
    "\n",
    "#     def nats_to_bits_per_dim(x):\n",
    "#         c, h, w = dataset_dims\n",
    "#         return autils.nats_to_bits_per_dim(x, c, h, w)\n",
    "\n",
    "#     _log.info('Starting training...')\n",
    "\n",
    "#     best_val_log_prob = None\n",
    "#     start_time = None\n",
    "#     num_batches = num_steps - start_step\n",
    "\n",
    "#     for step, (batch, _) in enumerate(load_num_batches(loader=train_loader,\n",
    "#                                                        num_batches=num_batches),\n",
    "#                                       start=start_step):\n",
    "#         if step == 0:\n",
    "#             start_time = time.time() # Runtime estimate will be more accurate if set here.\n",
    "\n",
    "#         flow.train()\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         batch = batch.to(device)\n",
    "\n",
    "#         if multi_gpu:\n",
    "#             if actnorm and step == 0:\n",
    "#                 # Is using actnorm, data-dependent initialization doesn't work with data_parallel,\n",
    "#                 # so pass a single batch on a single GPU before the first step.\n",
    "#                 flow.log_prob(\n",
    "#                     batch[:batch.shape[0] // torch.cuda.device_count(), ...]\n",
    "#                 )\n",
    "\n",
    "#             # Split along the batch dimension and put each split on a separate GPU. All available\n",
    "#             # GPUs are used.\n",
    "#             log_density = nn.parallel.data_parallel(LogProbWrapper(flow), batch)\n",
    "#         else:\n",
    "#             log_density = flow.log_prob(batch)\n",
    "\n",
    "#         loss = -nats_to_bits_per_dim(torch.mean(log_density))\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if scheduler is not None:\n",
    "#             scheduler.step()\n",
    "#             summary_writer.add_scalar('learning_rate', scheduler.get_lr()[0], step)\n",
    "\n",
    "#         summary_writer.add_scalar('loss', loss.item(), step)\n",
    "\n",
    "#         if best_val_log_prob:\n",
    "#             summary_writer.add_scalar('best_val_log_prob', best_val_log_prob, step)\n",
    "\n",
    "#         flow.eval() # Everything beyond this point is evaluation.\n",
    "\n",
    "#         if step % intervals['log'] == 0:\n",
    "#             elapsed_time = time.time() - start_time\n",
    "#             progress = autils.progress_string(elapsed_time, step, num_steps)\n",
    "#             _log.info(\"It: {}/{} loss: {:.3f} [{}]\".format(step, num_steps, loss, progress))\n",
    "\n",
    "#         if step % intervals['sample'] == 0:\n",
    "#             fig, axs = plt.subplots(1, len(temperatures), figsize=(4 * len(temperatures), 4))\n",
    "#             for temperature, ax in zip(temperatures, axs.flat):\n",
    "#                 with torch.no_grad():\n",
    "#                     noise = flow._distribution.sample(64) * temperature\n",
    "#                     samples, _ = flow._transform.inverse(noise)\n",
    "#                     samples = Preprocess(num_bits).inverse(samples)\n",
    "\n",
    "#                 autils.imshow(make_grid(samples, nrow=8), ax)\n",
    "\n",
    "#                 ax.set_title('T={:.2f}'.format(temperature))\n",
    "\n",
    "#             summary_writer.add_figure(tag='samples', figure=fig, global_step=step)\n",
    "\n",
    "#             plt.close(fig)\n",
    "\n",
    "#         if step > 0 and step % intervals['eval'] == 0 and (val_loader is not None):\n",
    "#             if multi_gpu:\n",
    "#                 def log_prob_fn(batch):\n",
    "#                     return nn.parallel.data_parallel(LogProbWrapper(flow),\n",
    "#                                                      batch.to(device))\n",
    "#             else:\n",
    "#                 def log_prob_fn(batch):\n",
    "#                     return flow.log_prob(batch.to(device))\n",
    "\n",
    "#             val_log_prob = autils.eval_log_density(log_prob_fn=log_prob_fn,\n",
    "#                                                    data_loader=val_loader)\n",
    "#             val_log_prob = nats_to_bits_per_dim(val_log_prob).item()\n",
    "\n",
    "#             _log.info(\"It: {}/{} val_log_prob: {:.3f}\".format(step, num_steps, val_log_prob))\n",
    "#             summary_writer.add_scalar('val_log_prob', val_log_prob, step)\n",
    "\n",
    "#             if best_val_log_prob is None or val_log_prob > best_val_log_prob:\n",
    "#                 best_val_log_prob = val_log_prob\n",
    "\n",
    "#                 torch.save(flow.state_dict(), os.path.join(run_dir, 'flow_best.pt'))\n",
    "#                 _log.info('It: {}/{} best val_log_prob improved, saved flow_best.pt'\n",
    "#                           .format(step, num_steps))\n",
    "\n",
    "#         if step > 0 and (step % intervals['save'] == 0 or step == (num_steps - 1)):\n",
    "#             torch.save(optimizer.state_dict(), os.path.join(run_dir, 'optimizer_last.pt'))\n",
    "#             torch.save(flow.state_dict(), os.path.join(run_dir, 'flow_last.pt'))\n",
    "#             _log.info('It: {}/{} saved optimizer_last.pt and flow_last.pt'.format(step, num_steps))\n",
    "\n",
    "#         if step > 0 and step % intervals['reconstruct'] == 0:\n",
    "#             with torch.no_grad():\n",
    "#                 random_batch_ = random_batch.to(device)\n",
    "#                 random_batch_rec, logabsdet = identity_transform(random_batch_)\n",
    "\n",
    "#                 max_abs_diff = torch.max(torch.abs(random_batch_rec - random_batch_))\n",
    "#                 max_logabsdet = torch.max(logabsdet)\n",
    "\n",
    "#             # fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "#             # autils.imshow(make_grid(Preprocess(num_bits).inverse(random_batch[:36, ...]),\n",
    "#             #                         nrow=6), axs[0])\n",
    "#             # autils.imshow(make_grid(Preprocess(num_bits).inverse(random_batch_rec[:36, ...]),\n",
    "#             #                         nrow=6), axs[1])\n",
    "#             # summary_writer.add_figure(tag='reconstr', figure=fig, global_step=step)\n",
    "#             # plt.close(fig)\n",
    "\n",
    "#             summary_writer.add_scalar(tag='max_reconstr_abs_diff',\n",
    "#                                       scalar_value=max_abs_diff.item(),\n",
    "#                                       global_step=step)\n",
    "#             summary_writer.add_scalar(tag='max_reconstr_logabsdet',\n",
    "#                                       scalar_value=max_logabsdet.item(),\n",
    "#                                       global_step=step)\n",
    "\n",
    "# # @ex.capture\n",
    "# def set_device(use_gpu, multi_gpu, _log):\n",
    "#     # Decide which device to use.\n",
    "#     if use_gpu and not torch.cuda.is_available():\n",
    "#         raise RuntimeError('use_gpu is True but CUDA is not available')\n",
    "\n",
    "#     if use_gpu:\n",
    "#         device = torch.device('cuda')\n",
    "#         torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "#     else:\n",
    "#         device = torch.device('cpu')\n",
    "\n",
    "#     if multi_gpu and torch.cuda.device_count() == 1:\n",
    "#         raise RuntimeError('Multiple GPU training requested, but only one GPU is available.')\n",
    "\n",
    "#     if multi_gpu:\n",
    "#         _log.info('Using all {} GPUs available'.format(torch.cuda.device_count()))\n",
    "\n",
    "#     return device\n",
    "\n",
    "# # @ex.capture\n",
    "# def get_train_valid_data(dataset, num_bits, valid_frac):\n",
    "#     return get_data(dataset, num_bits, train=True, valid_frac=valid_frac)\n",
    "\n",
    "# # @ex.capture\n",
    "# def get_test_data(dataset, num_bits):\n",
    "#     return get_data(dataset, num_bits, train=False)\n",
    "\n",
    "# # @ex.command\n",
    "# def sample_for_paper(seed):\n",
    "#     run_dir = fso.dir\n",
    "\n",
    "#     sample(output_path=os.path.join(run_dir, 'samples_small.png'),\n",
    "#            num_samples=30,\n",
    "#            samples_per_row=10)\n",
    "\n",
    "#     sample(output_path=os.path.join(run_dir, 'samples_big.png'),\n",
    "#            num_samples=100,\n",
    "#            samples_per_row=10,\n",
    "#            seed=seed + 1)\n",
    "\n",
    "# # @ex.command(unobserved=True)\n",
    "# def eval_on_test(batch_size, num_workers, seed, _log):\n",
    "#     torch.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "\n",
    "#     device = set_device()\n",
    "#     test_dataset, (c, h, w) = get_test_data()\n",
    "#     _log.info('Test dataset size: {}'.format(len(test_dataset)))\n",
    "#     _log.info('Image dimensions: {}x{}x{}'.format(c, h, w))\n",
    "\n",
    "#     flow = create_flow(c, h, w).to(device)\n",
    "\n",
    "#     flow.eval()\n",
    "\n",
    "#     def log_prob_fn(batch):\n",
    "#         return flow.log_prob(batch.to(device))\n",
    "\n",
    "#     test_loader=DataLoader(dataset=test_dataset,\n",
    "#                            batch_size=batch_size,\n",
    "#                            num_workers=num_workers)\n",
    "#     test_loader = tqdm(test_loader)\n",
    "\n",
    "#     mean, err = autils.eval_log_density_2(log_prob_fn=log_prob_fn,\n",
    "#                                           data_loader=test_loader,\n",
    "#                                           c=c, h=h, w=w)\n",
    "#     print('Test log probability (bits/dim): {:.2f} +/- {:.4f}'.format(mean, err))\n",
    "\n",
    "# # @ex.command(unobserved=True)\n",
    "# def sample(seed, num_bits, num_samples, samples_per_row, _log, output_path=None):\n",
    "#     torch.set_grad_enabled(False)\n",
    "\n",
    "#     if output_path is None:\n",
    "#         output_path = 'samples.png'\n",
    "\n",
    "#     torch.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "\n",
    "#     device = set_device()\n",
    "\n",
    "#     _, _, (c, h, w) = get_train_valid_data()\n",
    "\n",
    "#     flow = create_flow(c, h, w).to(device)\n",
    "#     flow.eval()\n",
    "\n",
    "#     preprocess = Preprocess(num_bits)\n",
    "\n",
    "#     samples = flow.sample(num_samples)\n",
    "#     samples = preprocess.inverse(samples)\n",
    "\n",
    "#     save_image(samples.cpu(), output_path,\n",
    "#                nrow=samples_per_row,\n",
    "#                padding=0)\n",
    "\n",
    "# # @ex.command(unobserved=True)\n",
    "# def num_params(_log):\n",
    "#     _, _, (c, h, w) = get_train_valid_data()\n",
    "#     # c, h, w = 3, 256, 256\n",
    "#     create_flow(c, h, w)\n",
    "\n",
    "# # @ex.command(unobserved=True)\n",
    "# def eval_reconstruct(num_bits, batch_size, seed, num_reconstruct_batches, _log, output_path=''):\n",
    "#     torch.set_grad_enabled(False)\n",
    "\n",
    "#     device = set_device()\n",
    "\n",
    "#     torch.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "\n",
    "#     train_dataset, _, (c, h, w) = get_train_valid_data()\n",
    "\n",
    "#     flow = create_flow(c, h, w).to(device)\n",
    "#     flow.eval()\n",
    "\n",
    "#     train_loader = DataLoader(\n",
    "#         dataset=train_dataset,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=True\n",
    "#     )\n",
    "\n",
    "#     identity_transform = transforms.CompositeTransform([\n",
    "#         flow._transform,\n",
    "#         transforms.InverseTransform(flow._transform)\n",
    "#     ])\n",
    "\n",
    "#     first_batch = True\n",
    "#     abs_diff = []\n",
    "#     for batch,_ in tqdm(load_num_batches(train_loader, num_reconstruct_batches),\n",
    "#                         total=num_reconstruct_batches):\n",
    "#         batch = batch.to(device)\n",
    "#         batch_rec, _ = identity_transform(batch)\n",
    "#         abs_diff.append(torch.abs(batch_rec - batch))\n",
    "\n",
    "#         if first_batch:\n",
    "#             batch = Preprocess(num_bits).inverse(batch[:36, ...])\n",
    "#             batch_rec = Preprocess(num_bits).inverse(batch_rec[:36, ...])\n",
    "\n",
    "#             save_image(batch.cpu(), os.path.join(output_path, 'invertibility_orig.png'),\n",
    "#                        nrow=6,\n",
    "#                        padding=0)\n",
    "\n",
    "#             save_image(batch_rec.cpu(), os.path.join(output_path, 'invertibility_rec.png'),\n",
    "#                        nrow=6,\n",
    "#                        padding=0)\n",
    "\n",
    "#             first_batch = False\n",
    "\n",
    "#     abs_diff = torch.cat(abs_diff)\n",
    "\n",
    "#     print('max abs diff: {:.4f}'.format(torch.max(abs_diff).item()))\n",
    "\n",
    "\n",
    "# # @ex.command(unobserved=True)\n",
    "# def profile(batch_size, num_workers):\n",
    "#     train_dataset, _, _ = get_train_valid_data()\n",
    "\n",
    "#     train_loader = DataLoader(dataset=train_dataset,\n",
    "#                               batch_size=batch_size,\n",
    "#                               num_workers=num_workers)\n",
    "#     for _ in tqdm(load_num_batches(train_loader, 1000),\n",
    "#                   total=1000):\n",
    "#         pass\n",
    "\n",
    "# # @ex.command(unobserved=True)\n",
    "# def plot_data(num_bits, num_samples, samples_per_row, seed):\n",
    "#     torch.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "\n",
    "#     train_dataset, _, _ = get_train_valid_data()\n",
    "\n",
    "#     samples = torch.cat([train_dataset[i][0] for i in np.random.randint(0, len(train_dataset),\n",
    "#                                                                         num_samples)])\n",
    "#     samples = Preprocess(num_bits).inverse(samples)\n",
    "\n",
    "#     save_image(samples.cpu(),\n",
    "#                'samples.png',\n",
    "#                nrow=samples_per_row,\n",
    "#                padding=0)\n",
    "\n",
    "# # @ex.automain\n",
    "# def main(seed, _log):\n",
    "#     torch.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "\n",
    "#     device = set_device()\n",
    "\n",
    "#     train_dataset, val_dataset, (c, h, w) = get_train_valid_data()\n",
    "\n",
    "#     # TODO change to use our datasets\n",
    "\n",
    "#     _log.info('Training dataset size: {}'.format(len(train_dataset)))\n",
    "\n",
    "#     if val_dataset is None:\n",
    "#         _log.info('No validation dataset')\n",
    "#     else:\n",
    "#         _log.info('Validation dataset size: {}'.format(len(val_dataset)))\n",
    "\n",
    "#     _log.info('Image dimensions: {}x{}x{}'.format(c, h, w))\n",
    "\n",
    "#     flow = create_flow(c, h, w)\n",
    "\n",
    "#     train_flow(flow, train_dataset, val_dataset, (c, h, w), device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837313f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c3c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2c40c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.net = nn.Sequential(\n",
    "            Conv2dSameSize(in_channels, hidden_channels, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            Conv2dSameSize(hidden_channels, hidden_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            Conv2dSameSize(hidden_channels, out_channels, kernel_size=3),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, context=None):\n",
    "        return self.net.forward(inputs)\n",
    "\n",
    "def create_transform_step(num_channels,\n",
    "                          hidden_channels, actnorm, coupling_layer_type, spline_params,\n",
    "                          use_resnet, num_res_blocks, resnet_batchnorm, dropout_prob):\n",
    "    if use_resnet:\n",
    "        def create_convnet(in_channels, out_channels):\n",
    "            net = nn_.ConvResidualNet(in_channels=in_channels,\n",
    "                                      out_channels=out_channels,\n",
    "                                      hidden_channels=hidden_channels,\n",
    "                                      num_blocks=num_res_blocks,\n",
    "                                      use_batch_norm=resnet_batchnorm,\n",
    "                                      dropout_probability=dropout_prob)\n",
    "            return net\n",
    "    else:\n",
    "        if dropout_prob != 0.:\n",
    "            raise ValueError()\n",
    "        def create_convnet(in_channels, out_channels):\n",
    "            return ConvNet(in_channels, hidden_channels, out_channels)\n",
    "\n",
    "    mask = utils.create_mid_split_binary_mask(num_channels)\n",
    "\n",
    "    if coupling_layer_type == 'cubic_spline':\n",
    "        coupling_layer = transforms.PiecewiseCubicCouplingTransform(\n",
    "            mask=mask,\n",
    "            transform_net_create_fn=create_convnet,\n",
    "            tails='linear',\n",
    "            tail_bound=spline_params['tail_bound'],\n",
    "            num_bins=spline_params['num_bins'],\n",
    "            apply_unconditional_transform=spline_params['apply_unconditional_transform'],\n",
    "            min_bin_width=spline_params['min_bin_width'],\n",
    "            min_bin_height=spline_params['min_bin_height']\n",
    "        )\n",
    "    elif coupling_layer_type == 'quadratic_spline':\n",
    "        coupling_layer = transforms.PiecewiseQuadraticCouplingTransform(\n",
    "            mask=mask,\n",
    "            transform_net_create_fn=create_convnet,\n",
    "            tails='linear',\n",
    "            tail_bound=spline_params['tail_bound'],\n",
    "            num_bins=spline_params['num_bins'],\n",
    "            apply_unconditional_transform=spline_params['apply_unconditional_transform'],\n",
    "            min_bin_width=spline_params['min_bin_width'],\n",
    "            min_bin_height=spline_params['min_bin_height']\n",
    "        )\n",
    "    elif coupling_layer_type == 'rational_quadratic_spline':\n",
    "        coupling_layer = transforms.PiecewiseRationalQuadraticCouplingTransform(\n",
    "            mask=mask,\n",
    "            transform_net_create_fn=create_convnet,\n",
    "            tails='linear',\n",
    "            tail_bound=spline_params['tail_bound'],\n",
    "            num_bins=spline_params['num_bins'],\n",
    "            apply_unconditional_transform=spline_params['apply_unconditional_transform'],\n",
    "            min_bin_width=spline_params['min_bin_width'],\n",
    "            min_bin_height=spline_params['min_bin_height'],\n",
    "            min_derivative=spline_params['min_derivative']\n",
    "        )\n",
    "    elif coupling_layer_type == 'affine':\n",
    "        coupling_layer = transforms.AffineCouplingTransform(\n",
    "            mask=mask,\n",
    "            transform_net_create_fn=create_convnet\n",
    "        )\n",
    "    elif coupling_layer_type == 'additive':\n",
    "        coupling_layer = transforms.AdditiveCouplingTransform(\n",
    "            mask=mask,\n",
    "            transform_net_create_fn=create_convnet\n",
    "        )\n",
    "    else:\n",
    "        raise RuntimeError('Unknown coupling_layer_type')\n",
    "\n",
    "    step_transforms = []\n",
    "\n",
    "    if actnorm:\n",
    "        step_transforms.append(transforms.ActNorm(num_channels))\n",
    "\n",
    "    step_transforms.extend([\n",
    "        transforms.OneByOneConvolution(num_channels),\n",
    "        coupling_layer\n",
    "    ])\n",
    "\n",
    "    return transforms.CompositeTransform(step_transforms)\n",
    "\n",
    "def create_transform(c, h, w,\n",
    "                     levels, hidden_channels, steps_per_level, alpha, num_bits, preprocessing,\n",
    "                     multi_scale, transform_step_kwargs: dict):\n",
    "    if not isinstance(hidden_channels, list):\n",
    "        hidden_channels = [hidden_channels] * levels\n",
    "\n",
    "    if multi_scale:\n",
    "        mct = transforms.MultiscaleCompositeTransform(num_transforms=levels)\n",
    "        for level, level_hidden_channels in zip(range(levels), hidden_channels):\n",
    "            squeeze_transform = transforms.SqueezeTransform()\n",
    "            c, h, w = squeeze_transform.get_output_shape(c, h, w)\n",
    "\n",
    "            transform_level = transforms.CompositeTransform(\n",
    "                [squeeze_transform]\n",
    "                + [create_transform_step(c, level_hidden_channels, **transform_step_kwargs) for _ in range(steps_per_level)]\n",
    "                + [transforms.OneByOneConvolution(c)] # End each level with a linear transformation.\n",
    "            )\n",
    "\n",
    "            new_shape = mct.add_transform(transform_level, (c, h, w))\n",
    "            if new_shape:  # If not last layer\n",
    "                c, h, w = new_shape\n",
    "    else:\n",
    "        all_transforms = []\n",
    "\n",
    "        for level, level_hidden_channels in zip(range(levels), hidden_channels):\n",
    "            squeeze_transform = transforms.SqueezeTransform()\n",
    "            c, h, w = squeeze_transform.get_output_shape(c, h, w)\n",
    "\n",
    "            transform_level = transforms.CompositeTransform(\n",
    "                [squeeze_transform]\n",
    "                + [create_transform_step(c, level_hidden_channels, **transform_step_kwargs) for _ in range(steps_per_level)]\n",
    "                + [transforms.OneByOneConvolution(c)] # End each level with a linear transformation.\n",
    "            )\n",
    "            all_transforms.append(transform_level)\n",
    "\n",
    "        all_transforms.append(transforms.ReshapeTransform(\n",
    "            input_shape=(c,h,w),\n",
    "            output_shape=(c*h*w,)\n",
    "        ))\n",
    "        mct = transforms.CompositeTransform(all_transforms)\n",
    "\n",
    "    # Inputs to the model in [0, 2 ** num_bits]\n",
    "\n",
    "    if preprocessing == 'glow':\n",
    "        # Map to [-0.5,0.5]\n",
    "        preprocess_transform = transforms.AffineScalarTransform(scale=(1. / 2 ** num_bits),\n",
    "                                                                shift=-0.5)\n",
    "    elif preprocessing == 'realnvp':\n",
    "        preprocess_transform = transforms.CompositeTransform([\n",
    "            # Map to [0,1]\n",
    "            transforms.AffineScalarTransform(scale=(1. / 2 ** num_bits)),\n",
    "            # Map into unconstrained space as done in RealNVP\n",
    "            transforms.AffineScalarTransform(shift=alpha,\n",
    "                                             scale=(1 - alpha)),\n",
    "            transforms.Logit()\n",
    "        ])\n",
    "\n",
    "    elif preprocessing == 'realnvp_2alpha':\n",
    "        preprocess_transform = transforms.CompositeTransform([\n",
    "            transforms.AffineScalarTransform(scale=(1. / 2 ** num_bits)),\n",
    "            transforms.AffineScalarTransform(shift=alpha,\n",
    "                                             scale=(1 - 2. * alpha)),\n",
    "            transforms.Logit()\n",
    "        ])\n",
    "    else:\n",
    "        raise RuntimeError('Unknown preprocessing type: {}'.format(preprocessing))\n",
    "\n",
    "    return transforms.CompositeTransform([preprocess_transform, mct])\n",
    "\n",
    "\n",
    "def create_flow(\n",
    "    c, h, w,\n",
    "    create_transform_kwargs: dict,\n",
    "    transform_step_kwargs: dict,\n",
    "    flow_checkpoint=None, \n",
    "    _log=logger,\n",
    "    ):\n",
    "    distribution = distributions.StandardNormal((c * h * w,))\n",
    "    transform = create_transform(\n",
    "        c, h, w, \n",
    "        **create_transform_kwargs,\n",
    "        transform_step_kwargs=transform_step_kwargs\n",
    "    )\n",
    "\n",
    "    flow = flows.Flow(transform, distribution)\n",
    "\n",
    "    _log.info('There are {} trainable parameters in this model.'.format(\n",
    "        utils.get_num_parameters(flow)))\n",
    "\n",
    "    if flow_checkpoint is not None:\n",
    "        flow.load_state_dict(torch.load(flow_checkpoint))\n",
    "        _log.info('Flow state loaded from {}'.format(flow_checkpoint))\n",
    "\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee54be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_flow(flow, train_dataset, val_dataset, (c, h, w), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3555a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-18 15:03:29.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcreate_flow\u001b[0m:\u001b[36m183\u001b[0m - \u001b[1mThere are 5858264 trainable parameters in this model.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "create_transform_kwargs = {\n",
    "    \"levels\": 3,\n",
    "    \"multi_scale\": True,\n",
    "    \"alpha\": 0.05,\n",
    "    \"num_bits\": 8,\n",
    "    \"steps_per_level\": 10,\n",
    "    \"hidden_channels\": 256,\n",
    "    \"preprocessing\": \"glow\",\n",
    "}\n",
    "transform_step_kwargs = {\n",
    "    \"actnorm\": True,\n",
    "    \"coupling_layer_type\": \"rational_quadratic_spline\",\n",
    "    \"spline_params\": {\n",
    "        \"num_bins\": 4,\n",
    "        \"tail_bound\": 1.0,\n",
    "        \"min_bin_width\": 1e-3,\n",
    "        \"min_bin_height\": 1e-3,\n",
    "        \"min_derivative\": 1e-3,\n",
    "        \"apply_unconditional_transform\": False,\n",
    "    },\n",
    "    \"use_resnet\": False,\n",
    "    \"num_res_blocks\": 5,\n",
    "    \"resnet_batchnorm\": True,\n",
    "    \"dropout_prob\": 0.0,\n",
    "}\n",
    "\n",
    "c = 1\n",
    "h = 64\n",
    "w = 64\n",
    "flow = create_flow(\n",
    "    c, h, w, \n",
    "    create_transform_kwargs=create_transform_kwargs,\n",
    "    transform_step_kwargs=transform_step_kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16d9eac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    def __init__(self):\n",
    "        # Dataset\n",
    "        self.dataset = 'fashion-mnist'\n",
    "        self.num_workers = 0\n",
    "        self.valid_frac = 0.01\n",
    "\n",
    "        # Pre-processing\n",
    "        self.preprocessing = 'glow'\n",
    "        self.alpha = .05\n",
    "        self.num_bits = 8\n",
    "        self.pad = 2 # For mnist-like datasets\n",
    "\n",
    "        # Model architecture\n",
    "        self.steps_per_level = 10\n",
    "        self.levels = 3\n",
    "        self.multi_scale=True\n",
    "        self.actnorm = True\n",
    "\n",
    "        # Coupling transform\n",
    "        self.coupling_layer_type = 'rational_quadratic_spline'\n",
    "        self.spline_params = {\n",
    "            'num_bins': 4,\n",
    "            'tail_bound': 1.,\n",
    "            'min_bin_width': 1e-3,\n",
    "            'min_bin_height': 1e-3,\n",
    "            'min_derivative': 1e-3,\n",
    "            'apply_unconditional_transform': False\n",
    "        }\n",
    "\n",
    "        # Coupling transform net\n",
    "        self.hidden_channels = 256\n",
    "        self.use_resnet = False\n",
    "        self.num_res_blocks = 5 # If using resnet\n",
    "        self.resnet_batchnorm = True\n",
    "        self.dropout_prob = 0.\n",
    "\n",
    "        # Optimization\n",
    "        self.batch_size = 256\n",
    "        self.learning_rate = 5e-4\n",
    "        self.cosine_annealing = True\n",
    "        self.eta_min=0.\n",
    "        self.warmup_fraction = 0.\n",
    "        self.num_steps = 100000\n",
    "        self.temperatures = [0.5, 0.75, 1.]\n",
    "\n",
    "        # Training logistics\n",
    "        self.use_gpu = True\n",
    "        self.multi_gpu = False\n",
    "        self.run_descr = ''\n",
    "        self.flow_checkpoint = None\n",
    "        self.optimizer_checkpoint = None\n",
    "        self.start_step = 0\n",
    "\n",
    "        self.intervals = {\n",
    "            'save': 1000,\n",
    "            'sample': 1000,\n",
    "            'eval': 1000,\n",
    "            'reconstruct': 1000,\n",
    "            'log': 10 # Very cheap.\n",
    "        }\n",
    "\n",
    "        # For evaluation\n",
    "        self.num_samples = 64\n",
    "        self.samples_per_row = 8\n",
    "        self.num_reconstruct_batches = 10\n",
    "\n",
    "cfg = config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff19a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f0fe959",
   "metadata": {},
   "source": [
    "## lightning version of train_flow (from chatgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f9cfeff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'experiments.autils' from '/home/b/b382145/nsf/experiments/autils.py'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28359986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/bb1153/b382145/cv_hydra_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m2025-07-18 15:20:28.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcreate_flow\u001b[0m:\u001b[36m183\u001b[0m - \u001b[1mThere are 5858264 trainable parameters in this model.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from lightning import LightningModule\n",
    "from experiments import autils\n",
    "from experiments.autils import Conv2dSameSize, LogProbWrapper\n",
    "\n",
    "DEFAULT_TRANSFORM_KWARGS = {\n",
    "    \"levels\": 3,\n",
    "    \"multi_scale\": True,\n",
    "    \"alpha\": 0.05,\n",
    "    \"num_bits\": 8,\n",
    "    \"steps_per_level\": 10,\n",
    "    \"hidden_channels\": 256,\n",
    "    \"preprocessing\": \"glow\",\n",
    "}\n",
    "DEFAULT_TRANSFORM_STEP_KWARGS = {\n",
    "    \"actnorm\": True,\n",
    "    \"coupling_layer_type\": \"rational_quadratic_spline\",\n",
    "    \"spline_params\": {\n",
    "        \"num_bins\": 4,\n",
    "        \"tail_bound\": 1.0,\n",
    "        \"min_bin_width\": 1e-3,\n",
    "        \"min_bin_height\": 1e-3,\n",
    "        \"min_derivative\": 1e-3,\n",
    "        \"apply_unconditional_transform\": False,\n",
    "    },\n",
    "    \"use_resnet\": False,\n",
    "    \"num_res_blocks\": 5,\n",
    "    \"resnet_batchnorm\": True,\n",
    "    \"dropout_prob\": 0.0,\n",
    "}\n",
    "\n",
    "class Flow(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dims: tuple = (1, 64, 64),\n",
    "        create_transform_kwargs: dict = DEFAULT_TRANSFORM_KWARGS,\n",
    "        transform_step_kwargs: dict = DEFAULT_TRANSFORM_STEP_KWARGS,\n",
    "        num_bits: int = 8,\n",
    "        learning_rate: float = 5e-4,\n",
    "        eta_min: float = 0.0,\n",
    "        cosine_annealing: bool = True,\n",
    "        temperatures: list = [0.5, 0.75, 1.0],\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        c, h, w = image_dims\n",
    "        self.flow = create_flow(\n",
    "            c, h, w, \n",
    "            create_transform_kwargs=create_transform_kwargs,\n",
    "            transform_step_kwargs=transform_step_kwargs,\n",
    "            )\n",
    "        self.num_bits = num_bits\n",
    "        self.learning_rate = learning_rate\n",
    "        self.eta_min = eta_min\n",
    "        self.cosine_annealing = cosine_annealing\n",
    "        self.temperatures = temperatures\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.flow.log_prob(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        log_prob = self.flow.log_prob(x)\n",
    "        loss = -autils.nats_to_bits_per_dim(log_prob.mean(), *self.dataset_dims)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        log_prob = self.flow.log_prob(x)\n",
    "        val_loss = -autils.nats_to_bits_per_dim(log_prob.mean(), *self.dataset_dims)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        return val_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        if self.cosine_annealing:\n",
    "            if self.warmup_fraction == 0.0:\n",
    "                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                    optimizer=optimizer,\n",
    "                    T_max=self.trainer.max_steps,\n",
    "                    eta_min=self.eta_min\n",
    "                )\n",
    "            else:\n",
    "                scheduler = optim.CosineAnnealingWarmUpLR(\n",
    "                    optimizer=optimizer,\n",
    "                    warm_up_epochs=int(self.warmup_fraction * self.trainer.max_steps),\n",
    "                    total_epochs=self.trainer.max_steps,\n",
    "                    eta_min=self.eta_min\n",
    "                )\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "        else:\n",
    "            return optimizer\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.current_epoch % self.sample_interval != 0:\n",
    "            return\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fig, axs = plt.subplots(1, len(self.temperatures), figsize=(4 * len(self.temperatures), 4))\n",
    "            for temperature, ax in zip(self.temperatures, axs.flat):\n",
    "                noise = self.flow._distribution.sample(64) * temperature\n",
    "                samples, _ = self.flow._transform.inverse(noise)\n",
    "                samples = Preprocess(self.num_bits).inverse(samples) # TODO implement Preprocess for OLR\n",
    "                autils.imshow(make_grid(samples, nrow=8), ax)\n",
    "                ax.set_title(f'T={temperature:.2f}')\n",
    "            \n",
    "            self.logger.experiment.add_figure(\"samples\", fig, global_step=self.global_step)\n",
    "            plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61009a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/bb1153/b382145/cv_hydra_env/lib/python3.11/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /work/bb1153/b382145/cv_hydra_env/lib/python3.11/sit ...\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\u001b[32m2025-07-18 15:20:42.644\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcreate_flow\u001b[0m:\u001b[36m183\u001b[0m - \u001b[1mThere are 5858264 trainable parameters in this model.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# Logger\n",
    "logger = TensorBoardLogger(\"runs\", name=\"flow_experiment\")\n",
    "\n",
    "# Checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    save_top_k=1,\n",
    "    mode=\"min\",\n",
    "    filename=\"flow-best-{epoch}-{val_loss:.2f}\"\n",
    ")\n",
    "\n",
    "# LR logging\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "# TODO set these in hydra config:\n",
    "num_steps = 100\n",
    "intervals = {\n",
    "            'save': 1000,\n",
    "            'sample': 1000,\n",
    "            'eval': 1000,\n",
    "            'reconstruct': 1000,\n",
    "            'log': 10 # Very cheap.\n",
    "        }\n",
    "\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback, lr_monitor],\n",
    "    max_steps=num_steps,\n",
    "    val_check_interval=intervals['eval'],\n",
    "    log_every_n_steps=intervals['log'],\n",
    "    check_val_every_n_epoch=None,  # We're validating on step intervals instead\n",
    "    devices=\"auto\",  # Or set manually\n",
    "    accelerator=\"auto\",\n",
    "    precision=32,  # Or 16 for mixed precision\n",
    ")\n",
    "\n",
    "# Instantiate model\n",
    "model = Flow()\n",
    "\n",
    "# Train\n",
    "# trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader) # TODO adapt our dataloaders as needed for this model (preprocessing etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb4bddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
